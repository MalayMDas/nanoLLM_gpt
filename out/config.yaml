always_save_checkpoint: true
backend: nccl
batch_size: 12
beta1: 0.9
beta2: 0.95
compile: false
data_path: uploads\input.txt
dataset: custom
decay_lr: true
device: cuda
dtype: bfloat16
eval_interval: 50
eval_iters: 200
eval_only: false
grad_clip: 1.0
gradient_accumulation_steps: 40
init_from: resume
learning_rate: 1.0e-05
log_interval: 1
lr_decay_iters: 600000
max_iters: 2000
min_lr: 6.0e-05
model:
  bias: true
  block_size: 64
  dropout: 0.0
  n_embd: 64
  n_head: 8
  n_layer: 6
  vocab_size: 50304
out_dir: out
seed: 1337
train_val_split: 0.0005
wandb_log: false
wandb_project: gpt-training
wandb_run_name: gpt-run
warmup_iters: 2000
weight_decay: 0.1
